{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b69566-609b-4f10-93f5-cafad8ba7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.insert(0, './Transformer/')\n",
    "from model import LabelSmoothing,NoamOpt,SimpleLossCompute,make_model,run_epoch\n",
    "from util import Batch\n",
    "from util import greedy_decode\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Transformer Task')\n",
    "parser.add_argument('--task', type=str, default='phi',choices=['RMSD','phi','psi'],help='choose dataset as phi/psi/RMSD')\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--interval', type=int, default=10,help='saving interval ')\n",
    "\n",
    "parser.add_argument('--seq_length', type=int, default=100)\n",
    "\n",
    "parser.add_argument('--unidirection', type=bool, default=False,help='use mask to do unidirection encoding in transformer')\n",
    "\n",
    "parser.add_argument('--gpu_id', type=str, default='cuda:3')\n",
    "\n",
    "args,_ = parser.parse_known_args()\n",
    "task=args.task\n",
    "batch_size=args.batch_size\n",
    "interval=args.interval\n",
    "device= torch.device(args.gpu_id)\n",
    "uni=args.unidirection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eee0fa8-2bb7-42f8-8dcc-20f02a915c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "if task=='RMSD':\n",
    "    train,valid=np.loadtxt('data/alanine/train',dtype=int),np.loadtxt('data/alanine/valid',dtype=int)\n",
    "elif task=='phi':\n",
    "    train,valid=np.loadtxt('data/phi-psi/train_phi_0.1ps',dtype=int).reshape(-1),np.loadtxt('data/phi-psi/valid_phi_0.1ps',dtype=int).reshape(-1)\n",
    "elif task=='psi':\n",
    "    train,valid=np.loadtxt('data/phi-psi/train_psi_0.1ps',dtype=int).reshape(-1),np.loadtxt('data/phi-psi/valid_psi_0.10ps',dtype=int).reshape(-1)\n",
    "\n",
    "# subsample x to corresponding interval\n",
    "train=train.reshape(interval,-1).T.flatten().reshape(-1,100)\n",
    "valid=valid.reshape(interval,-1).T.flatten().reshape(-1,100)\n",
    "    \n",
    "# make dirs for saving files\n",
    "\n",
    "log_dir=\"Transformer/logs/fit/{}/interval{}_batch{}_uni_{}/\".format(task,interval,batch_size,uni)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "save_dir = \"Transformer/result/{}/interval{}_batch{}_uni_{}/\".format(task,interval,batch_size,uni)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "ckpt_dir=\"Transformer/ckpt/training_checkpoints_{}/interval{}_batch{}_uni_{}/\".format(task,interval,batch_size,uni)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce4c6d6-3e9a-4975-a3f9-403e111cbe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "./Transformer/model.py:237: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 3.819704 Tokens per Sec: 33100.718750\n",
      "Epoch Step: 501 Loss: 1.734601 Tokens per Sec: 50168.656250\n",
      "Epoch Step: 1001 Loss: 1.267899 Tokens per Sec: 48825.652344\n",
      "Epoch Step: 1501 Loss: 1.670407 Tokens per Sec: 48801.527344\n",
      "Epoch Step: 2001 Loss: 1.377584 Tokens per Sec: 48774.453125\n",
      "Epoch  0  Time: 162.169120\n",
      "Epoch Step: 1 Loss: 1.792161 Tokens per Sec: 71030.992188\n",
      "Epoch Step: 501 Loss: 1.255619 Tokens per Sec: 71237.437500\n",
      "Epoch Step: 1 Loss: 1.535694 Tokens per Sec: 51790.410156\n",
      "Epoch Step: 501 Loss: 1.320524 Tokens per Sec: 54715.546875\n",
      "Epoch Step: 1001 Loss: 1.135586 Tokens per Sec: 54806.878906\n",
      "Epoch Step: 1501 Loss: 0.934211 Tokens per Sec: 49880.320312\n",
      "Epoch Step: 2001 Loss: 0.859769 Tokens per Sec: 49687.105469\n",
      "Epoch  1  Time: 153.367423\n",
      "Epoch Step: 1 Loss: 0.819038 Tokens per Sec: 71843.453125\n",
      "Epoch Step: 501 Loss: 0.550437 Tokens per Sec: 70859.617188\n",
      "Epoch Step: 1 Loss: 2.062551 Tokens per Sec: 48380.484375\n",
      "Epoch Step: 501 Loss: 1.027535 Tokens per Sec: 49037.203125\n",
      "Epoch Step: 1001 Loss: 0.531182 Tokens per Sec: 49461.964844\n",
      "Epoch Step: 1501 Loss: 0.497641 Tokens per Sec: 48946.496094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d2c5bbf577b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss_train=run_epoch(data_generator(train, batch_size, pad), model, \n\u001b[0;32m---> 28\u001b[0;31m               SimpleLossCompute(model.generator, criterion, model_opt,device))\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     print(\"Epoch  %d  Time: %f\" %\n",
      "\u001b[0;32m/import/home/wzengad/local/code4git/Transformer/model.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m    247\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m    248\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/import/home/wzengad/local/code4git/Transformer/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m    330\u001b[0m         loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n\u001b[1;32m    331\u001b[0m                               y.contiguous().view(-1)) / norm   #why devide norm\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=5\n",
    "num_generate=10\n",
    "# -------training--------------\n",
    "def data_generator(fulldata, batch,pad):\n",
    "    \"Generate random data for a src copy task.\"\n",
    "    nbatches=int(fulldata.shape[0]//batch)\n",
    "    choice=[i for i in range(nbatches)]  \n",
    "    random.shuffle(choice)\n",
    "    for i in choice:\n",
    "        data=fulldata[i*batch:(i+1)*batch]      \n",
    "        src = Variable(torch.from_numpy(data), requires_grad=False)\n",
    "        yield Batch(src.to(device), src.to(device), pad,uni=uni)\n",
    "\n",
    "V = len(np.unique(train))\n",
    "pad=V+1\n",
    "criterion = LabelSmoothing(size=V, padding_idx=pad, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model.to(device)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    loss_train=run_epoch(data_generator(train, batch_size, pad), model, \n",
    "              SimpleLossCompute(model.generator, criterion, model_opt,device))\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Epoch  %d  Time: %f\" %\n",
    "                    (epoch, elapsed))\n",
    "    model.eval()\n",
    "    loss_valid=run_epoch(data_generator(valid, batch_size, pad), model, \n",
    "                    SimpleLossCompute(model.generator, criterion, None,device))\n",
    "    if epoch<5 or epoch%5==0:\n",
    "        torch.save(model.state_dict(), ckpt_dir+'epoch{}.pt'.format(epoch))\n",
    "    writer.add_scalar('Loss/train', loss_train, epoch)\n",
    "    writer.add_scalar('Loss/test', loss_valid, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bbd8e87-a60f-40d6-aae2-724b25453fc9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  73 | elapsed:   11.0s remaining:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  73 | elapsed:   11.0s remaining:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  73 | elapsed:   11.0s remaining:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  73 | elapsed:   11.0s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  73 | elapsed:   11.0s remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  73 | elapsed:   11.0s remaining:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  73 | elapsed:   11.0s remaining:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  73 | elapsed:   11.0s remaining:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  73 | elapsed:   11.0s remaining:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  73 | elapsed:   11.0s remaining:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  73 | elapsed:   11.0s remaining:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  73 | elapsed:   11.0s remaining:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  73 | elapsed:   11.0s remaining:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  73 | elapsed:   11.0s remaining:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  73 | elapsed:   11.0s remaining:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  73 | elapsed:   11.0s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  73 | elapsed:   11.0s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  73 | elapsed:   11.0s remaining:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  73 | elapsed:   11.0s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  73 | elapsed:   11.0s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  73 | elapsed:   11.0s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  73 | elapsed:   11.0s remaining:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  73 | elapsed:   11.0s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  73 | elapsed:   11.0s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  73 | elapsed:   11.0s remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  73 | elapsed:   11.0s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  73 | elapsed:   11.0s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  73 | elapsed:   11.0s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  73 | elapsed:   11.0s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  73 | elapsed:   11.0s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  73 | elapsed:   11.0s remaining:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of  73 | elapsed:   11.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of  73 | elapsed:   11.0s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  73 | elapsed:   11.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  73 | elapsed:   11.0s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  73 | elapsed:   11.0s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  73 | elapsed:   11.0s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of  73 | elapsed:   11.0s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  73 | elapsed:   11.0s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  73 | elapsed:   11.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  73 | elapsed:   11.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of  73 | elapsed:   11.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:   11.0s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 764.00 MiB (GPU 3; 10.76 GiB total capacity; 204.34 MiB already allocated; 35.44 MiB free; 226.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-8-75b672068937>\", line 10, in single_generation\n  File \"./Transformer/util.py\", line 55, in greedy_decode\n    memory = model.encode(src, src_mask)\n  File \"./Transformer/model.py\", line 31, in encode\n    return self.encoder(self.src_embed(src), src_mask)\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"./Transformer/model.py\", line 71, in forward\n    x = layer(x, mask)\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"./Transformer/model.py\", line 100, in forward\n    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"./Transformer/model.py\", line 87, in forward\n    return x + self.dropout(sublayer(self.norm(x)))\n  File \"./Transformer/model.py\", line 100, in <lambda>\n    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n  File \"/home/wzengad/.conda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"./Transformer/model.py\", line 169, in forward\n    dropout=self.dropout)\n  File \"./Transformer/model.py\", line 134, in attention\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\nRuntimeError: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 3; 10.76 GiB total capacity; 204.34 MiB already allocated; 35.44 MiB free; 226.00 MiB reserved in total by PyTorch)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-75b672068937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# cannot paralleled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_generation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 3; 10.76 GiB total capacity; 204.34 MiB already allocated; 35.44 MiB free; 226.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "def single_generation(i):\n",
    "    seg=int(len(train.reshape(-1))/100)\n",
    "    text4activation=train.reshape(-1)[i*seg:(i+1)*seg]  \n",
    "    start0 = time.time()\n",
    "    src = Variable(torch.from_numpy(text4activation[-5000:]).unsqueeze(0)).to(device)\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    start_symbol=src[-1][-1]\n",
    "\n",
    "    prediction=greedy_decode(model, src, src_mask, max_len=num_generate, start_symbol=start_symbol,pad=pad)\n",
    "    print ('Time taken for total {} sec\\n'.format(time.time() - start0))\n",
    "    save=save_dir+'epoch{}/'.format(epoch)\n",
    "    os.makedirs(save, exist_ok=True)\n",
    "    np.savetxt(save+'prediction_'+str(i),prediction.cpu(),fmt='%i')\n",
    "\n",
    "#for epoch in [2,5,90,100]:\n",
    "epoch_pre=1\n",
    "model.load_state_dict(torch.load(ckpt_dir+'epoch{}.pt'.format(epoch_pre)))\n",
    "'''for i in range(100):\n",
    "    single generation(i)'''\n",
    "    \n",
    "# cannot paralleled    \n",
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=-1, verbose=100)(delayed(single_generation)(i) for i in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2767f5e-704b-4d46-9e8d-6d61af49dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
